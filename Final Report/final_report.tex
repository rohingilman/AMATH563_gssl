\documentclass[12pt]{amsart}
\usepackage[top = 1in, bottom = 1in, left =1in, right = 1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{multimedia} % to embed movies in the PDF file
\usepackage{graphicx}
\usepackage{comment}
\usepackage[english]{babel}
\usepackage{amsfonts,amscd,amssymb,amsmath,mathrsfs}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{verbatim,cite}
\usepackage{float}
\usepackage{cancel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathdots}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{empheq}

\usepackage{macros}

\newcommand{\iid}{\overset{\mathrm{iid}}{\sim}}


\title{GSSL Project Report (should change this)}

\author{Rohin Gilman \and Alex Johnson \and Kaitlynn Lilly}


\date{June 9, 2023}


\doublespacing
\begin{document}

\begin{abstract}
	To be written last
\end{abstract}

\maketitle

\section{Introduction} 
Graphical semi-supervised learning (GSSL) is a prominent field of research in machine learning as it serves as a powerful technique for dealing with classification problems in which only a small subset of the data is labeled. \textcolor{red}{Incorporating kernel methods into GSSL has emerged as a popular approach in machine learning due to its ability to leverage both labeled and unlabeled data, resulting in improved accuracy and robustness. In kernel methods, the unlabeled data is used to construct a similarity graph, where each node represents an instance and the edges indicate the similarity between them. Clustering this graph results in a complete labeling of the data set.} As a result, graphical semi-supervised learning using kernel methods is a promising approach for tackling real-world problems in various domains such as image recognition, natural language processing, and bioinformatics.

\section{Theoretical Background}

\subsection{Types of Graphs}
\subsubsection{Proximity Graphs}
Given a set of $n$ points $\{x_1, x_2, \dots, x_n\}$ in a metric space, a proximity graph is a graph, $G = (V, W)$, where $V = \{v_1, v_2, \dots, v_n\}$ is a set of vertices representing the points, and $W$ is an adjacency matrix whose elements represent the weights of the edges connecting pairs of vertices that are ``close," according to some measure of proximity.

To represent the proximity graph $G$ using a weight matrix, we define a non-increasing kernel function $K\colon X \times X \rightarrow \mathbb{R}$, which captures the similarity between the points. The weight matrix, $W$, corresponding to the proximity graph $G$ is then defined by $W_{i,j} = K(x_i,x_j)$. The nonzero entries correspond to the similarity between pairs of points that are connected by an edge in $G$. Diagonal entries, $W_{i,i}$, correspond to the ``self-similarity" of each point $x_i$, and the off-diagonal entries, $W_{i,j}$, correspond to the similarity between pairs of points $x_i$ and $x_j$. By using a kernel function to define the weights of the edges, proximity graphs can capture underlying geometric relationships between the data points, making them suitable for a wide range of applications in machine learning and data analysis.
\textcolor{red}{We need to include an equation that shows the relationship of a radius in constructing the Proximity graph, with maybe some short explanation as to that radius}
\subsubsection{K-Nearest Neighbor}
K-Nearest Neighbor (K-NN) graphs are a type of proximity graph for which edge weights are determined exclusively by the nearest neighbors of a given vertex. Given a set of $n$ vertices $V = \{x_1, x_2, ..., x_n\}$ in a metric space, the K-NN graph $G_k = (V, W)$ is constructed as follows: for each point $x_i$, its $k$-nearest neighbors are found according to some distance metric, and edges of weight $1$ are added between $x_i$ and each of its $k$-nearest neighbors. In other words, $x_i$ is adjacent to the $k$ vertices that are closest to it in the data set.
\textcolor{red}{Include more than just the uniform kernel, really assign the weights according to a non-increasing weight function}

K-NN graphs have several advantages over other types of proximity graphs. One of these advantages is that on average, vertices have relatively low degree, so the weight matrix $W$ is often sparse. This increases the performance of graphical methods. Moreover, K-NN graphs are easy to construct and can be used in a wide range of applications, such as clustering, classification, and anomaly detection.

\subsection{Graph Laplacian}
Given a graph $G = (V, W)$, we can define a graph Laplacian $L$ as a matrix that captures the pairwise relationships between data points on the graph. Specifically, we can construct the unnormalized graph Laplacian $L$ as follows:
$$ L = D - W, $$
where $D$ is the diagonal degree matrix. The degree of a vertex is the sum of the weights of the edges adjacent to the vertex. We note that if $G$ has $M$ connected components, then $\mathrm{dim}(\mathrm{Null}(L))=M$. Thus, we simply need to determine the multiplicity of the zero eigenvalue of $L$ to determine the number of clusters in the data.

One common variant of the graph Laplacian is the normalized Laplacian, which is defined as:
$$ L_{\text{sym}} = D^{-1/2} L D^{-1/2}. $$
The normalized Laplacian satisfies the important property that its eigenvalues are in the range $[0, 2]$, which allows for efficient algorithms and analysis. Though there are desired properties, for our applications, the unnormalized Laplacian is sufficient and easier to compute.
\begin{comment}
In GSSL, we can use the Laplacian to propagate labels from a small set of labeled data points to the entire graph, by solving the following optimization problem:
$$ \min_f \sum_i L_{ii} (f_i - y_i)^2 + \lambda \sum_{i,j} W_{ij} (f_i - f_j)^2, $$
where $y_i$ is the label for the $i$-th labeled point, $f_i$ is the predicted label for the $i$-th point, and $\lambda$ is a regularization parameter that controls the smoothness of the solution. 
\end{comment}

\subsection{Types of Kernels}
\subsubsection{Mat\'{e}rn Family}

\subsection{GSSL Algorithms}
\subsubsection{The Probit Method}
The probit method is a statistical technique used for modeling binary response variables. We assume
\[y_j = \mathrm{sign}(f(x_j) + \epsilon_j),\]
for some latent function $f$, where $\epsilon_j\iid \psi$, for some probability density $\psi$. Assume $\psi$ is symmetric, so that
\begin{align*}
	\mathbb{P}(y_j = +1 \mid f) &= \mathbb{P}(f(x_j) + \epsilon_j \geq 0) \\
	&= \mathbb{P}(\epsilon_j \geq - f(x_j)  ) \\
	&= \int_{-f(x_j)}^\infty \psi(t) \ddd t \\
	&= \int_{-\infty}^{f(x_j)} \psi(t) \ddd t \\
	&= \Psi(f(x_j)) \\
	&= \Psi(y_jf(x_j)),
\end{align*}
where $\Psi$ is the cumulative distribution function (CDF) of $\psi$. By the same calculation, we obtain $\mathbb{P}(y_j = -1 \mid f) = \Psi(y_jf(x_j))$. Thus, $\mathbb{P}(y_j \mid f) = \Psi(y_jf(x_j))$. Since the $\epsilon_j$ are independent and identically distributed, we can write
\[\mathbb{P}(y  \mid f) = \prod_{j = 1}^n \Psi(y_jf(x_j)).\]
For a function $f$, we want our loss function to be large when the signs of $f(x_j)$ and $y_j$  disagree, so we define our probit loss:
\[L(f) = -\log(\mathbb{P}(y\mid f)) = -\sum_{j = 1}^n \Psi(y_jf(x_j)).\]
Finally, we add a regularization term based on the RKHS norm to define our optimization problem:
\[f^* = \argmin_{f \in \R^{m}} \mathcal{L}(f,y) + \lambda f^{T}C^{-1}f\]


\begin{comment}
In the kernel-based formulation of the probit method, the goal is to find the optimal values of the model coefficients, or dual variables, by maximizing the likelihood function. The likelihood function is defined as the product of the conditional probabilities of the response variable given the explanatory variables, which are modeled as a probit link function of a linear combination of kernel functions evaluated at the explanatory variables:
$$\mathbb{P}_r(y_i = 1 | \mathbf{x}_i) = \Phi(\mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}_i)),$$
where $y_i$ is the response variable for the $i$-th observation, $\mathbf{x}_i$ is the vector of explanatory variables, $\boldsymbol{\phi}(\cdot)$ is a nonlinear mapping of the input space to a high-dimensional feature space, $\mathbf{w}$ is a vector of model coefficients, and $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution. The model coefficients are then estimated by solving an optimization problem of the form:
$$\min_{\mathbf{w}} \left[- \sum_{i=1}^n y_i \log \Phi(\mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}_i)) + (1 - y_i) \log (1 - \Phi(\mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}_i))) + \frac{\lambda}{2} \Vert \mathbf{w} \Vert^2\right],$$
where the first two terms correspond to the negative log-likelihood function, the third term is a regularization term that controls the complexity of the model, and $\lambda$ is the regularization parameter. This optimization problem can be solved using kernel ridge regression, which involves minimizing a regularized empirical risk function of the form:
$$\min_{\boldsymbol{\alpha}} \frac{1}{n} \sum_{i=1}^n \ell(y_i, f(\mathbf{x}_i)) + \frac{\lambda}{2} \boldsymbol{\alpha}^\top \mathbf{K} \boldsymbol{\alpha},$$
where $\boldsymbol{\alpha}$ are the dual variables, $f(\mathbf{x}_i) = \sum_{j=1}^n \alpha_j K(\mathbf{x}_i, \mathbf{x}_j)$ is the predicted probability for the $i$-th observation, $\ell(y_i, f(\mathbf{x}_i))$ is the loss function that measures the discrepancy between the observed response $y_i$ and the predicted probability $f(\mathbf{x}_i)$, $\lambda$ is the regularization parameter that controls the trade-off between fitting the data and the complexity of the model, and $\mathbf{K}$ is the kernel matrix whose entries are given by $K(\mathbf{x}_i, \mathbf{x}_j) = \boldsymbol{\phi}(\mathbf{x}_i)^\top \boldsymbol{\phi}(\mathbf{x}_j)$. 

\subsubsection{Spectral Clustering}
Spectral clustering is a clustering technique that uses the eigenvectors of a matrix derived from the data to cluster the data points. One common approach is to use the Laplacian matrix of the graph defined by the data points. Once we have the Laplacian matrix, we can compute its eigenvectors and eigenvalues. The eigenvectors corresponding to the smallest eigenvalues (excluding the first eigenvector, which is constant) are used as the embedding of the data points. The embedding maps the data points to a low-dimensional space, where the pairwise distances reflect the underlying structure of the data. Finally, we can use a clustering algorithm (e.g., k-means) to cluster the data points in the embedding space. The number of clusters can be determined using the number of zero eigenvalues of the graph Laplacian.

\end{comment}

\section{Methods}
Write about what it is we are actually going to do

\section{Methods}
In this section, we describe our process for performing GSSL on several subproblems.

\subsection{General Procedure}
\begin{enumerate}
    \item Data is obtained on a given manifold. True labels for the data are either extracted or assigned and are ultimately used to compute the accuracy of our approach.
    \item A graph is constructed from the unlabelled data, used to infer structure on the data. 
    \item The minimal latent function is determined by solving an optimization problem over the sum of a loss function using only the known labels and regularization term.
    \item The minimal latent function is used to predict the labels on all unlabelled vertices.
    \item The accuracy of the predicted labels using the true labels is computed.
\end{enumerate}

\subsection{Subproblems}

We now perform our general GSSL procedure on three different subproblems: three well separated clusters on the $\mathbb{R}^2$ manifold, three clusters on the swiss roll manifold, and the clusters generated using two digits from the MNIST data set. For each subproblem, we perform GSSL on a disconnected and weakly connected graph, except for MNIST, on which we only use a weakly connected graph. We compute our minimal latent function using probit and regression loss.

\section{Results}

\subsection{Clusters on $\mathbb{R}^2$}\label{Sec:3Clus}

We begin by generating the points to form well-separated clusters in $\mathbb{R}^2$. These clusters are formed by specifying the centers of the clusters, the number of points in each distribution, and a covariance matrix. We demonstrate our results by choosing 3 clusters centered around $(0,0)$, $(1,0)$, and $(1,1)$, with 100 points in each cluster.
The points in the cluster centered around $(0,0)$ have true label $+1$, while the points in the other two clusters have true label $-1$. One point in each cluster is then assigned its true label. Only these three assigned points will be used in our GSSL loss function. A plot of the clusters formed using this process is shown in Figure~\ref{Fig:Dis3ClusOg}.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Dis3ClusOG.png}
    \caption{
        Plot of the 3 well-separated clusters; the yellow points represent a label of $+1$, while purple points represent a label of $-1$. The red points indicate the points for which the label is known. 
    }
	\label{Fig:Dis3ClusOg}
\end{figure}

We next use the data to construct a graph on the points. There are several choices to be made here, including the choice of weight functions, parameters, and type of graph. In this case, we explore two types of graphs: a K-NN approach and a full proximity graph approach. We begin by examining a K-NN approach using a uniform kernel. This allows us to create a disconnected graph with 3 connected clusters. Such a graph constructed using this approach is shown in Figure~\ref{Fig:Dis3ClusKNN}. Note that blue points indicate an unknown label, while red labels indicate known labels.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Dis3ClusKNN.png}
    \caption{
        Plot of the graph constructed on the unlabelled data from Figure~\ref{Fig:Dis3ClusOg} when using a K-NN approach with a uniform kernel. Blue points indicate an unknown label, while red labels indicate known labels. 
    }
	\label{Fig:Dis3ClusKNN}
\end{figure}
We then perform our GSSL algorithm with both the Probit loss function as described in Section~\ref{Sec:Probit} and the regression loss function, which is given by 
\[L(f)=\sum_{j}(y_j-f_j)^2.\]
We note that since the clusters in this case are completely disconnected that it was not necessary to tune any parameters in order to achieve an accuracy of 100\% using both loss functions. For each of these loss functions, we choose the following parameters: $\tau=1$, $\alpha=2$, and $\lambda=\frac{\tau^{2\alpha}}{2}$. Plots of the predicted labels using both the probit and regression loss functions are shown in Figure~\ref{Fig:Dis3ClusKNNRes}.
    \begin{figure}[ht] 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Dis3ClusKNNPro.png} 
    \caption{Probit Loss Function} 
    \label{Fig:Dis3ClusKNNPro} 
  \end{subfigure}%% 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Dis3ClusKNNReg.png} 
    \caption{Regression Loss Function} 
    \label{Fig:Dis3ClusKNNReg} 
  \end{subfigure} 
  \caption{Plot of the predicted labels for the K-NN graph in Figure~\ref{Fig:Dis3ClusKNN} and proximity graph in Figure~\ref{Fig:Dis3ClusProx}
  using only the known labels.}
  \label{Fig:Dis3ClusKNNRes}
\end{figure}
From Figure~\ref{Fig:Dis3ClusKNNRes}, we can see that our predicted labels are exactly those as our known labels, indicating that our GSSL was successful in this case. 

We now repeat the process with a full proximity graph approach. In this case, we construct a proximity graph using the RBF kernel. This allows us to create a fully connected graph in which the connections between the clusters are of $O(\epsilon)$. Usage of the RBF kernel requires tuning the $\gamma$ parameter to obtain the best results. We accomplish this with cross validation on the eigenvalues of the resulting graph Laplacian matrix. Since we expect three distinct clusters, there should be a large gap between the third and fourth eigenvalues. With the RBF kernel tuned, the proximity graph is shown in Figure~\ref{Fig:Dis3ClusProx} is produced.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Dis3ClusProx.png}
    \caption{
        Plot of the graph constructed on the unlabelled data from Figure~\ref{Fig:Dis3ClusOg} when using a full proximity graph approach with the RBF kernel. Note that the darker the shade of the edge, the higher its weight is. Note that this graph is fully connected with weights between clusters that are difficult to make out due to them having weight of $O(\epsilon)$.
    }
	\label{Fig:Dis3ClusProx}
\end{figure}

We then perform our GSSL algorithm. Here, it was necessary to tune the $\tau$ parameter, so that it is $O(\sqrt{\epsilon})$. Doing this resulted in 100\% accuracy for both loss functions and thus produced the same plots as shown in Figure~\ref{Fig:Dis3ClusKNNRes}.

To ensure that our accuracy results were consistent, we computed the average accuracy over 50 trials of each of the cases described above. The results of this are shown in Table~\ref{Table:3ClusRes}.
\begin{table}
\begin{center}
\begin{tabular}{||c c c||} 
 \hline
 Type of Graph & Loss Function & Average Accuracy\\  
 \hline\hline
 Disconnected (K-NN) & Probit & 100\% \\ 
 \hline
 Disconnected (K-NN) & Regression & 100\% \\
 \hline
 Weakly Connected (Proximity) & Probit & 100\% \\
 \hline
 Weakly Connected (Proximity) & Regression & 100\% \\ 
 \hline
\end{tabular}
\caption{Average Accuracy of predicted labels over 50 runs for both a fully disconnected and a weakly connected graph using both probit and regression loss functions.}
\label{Table:3ClusRes}
\end{center}
\end{table}

\subsection{Clusters on the Swiss Roll}

Consider the swiss roll manifold described by $(x,y) = (t\cos(t), t\sin(t))$. We begin by generating points to form well-separated clusters on this manifold. These clusters are formed by specifying a range of $t$-values for which each cluster will be defined, a variance for how far the points will vary from the true manifold, and the number of points to be generated. We demonstrate our results by choosing 3 clusters in the $t$-ranges $[\pi,5\pi/2]$, $[3\pi,7\pi/2]$, and $[4\pi,9\pi/2]$, respectively with a variance of 1. The cluster from $t=[\pi,5\pi/2]$ has 200 points and have true label $+1$, while the other two clusters each have 100 points and have true label $-1$. Two points in each cluster are then assigned its true label. We then further assigned two more points in the cluster labelled $+1$ their true label to ensure that each label is assigned equal numbers of true labels. This is done to avoid majority labelling, which resulted in poor accuracy when not accounted for. A plot of the clusters formed using this process is shown in Figure~\ref{Fig:SpiOg}.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/SpiOg.png}
    \caption{
        Plot of the 3 clusters on the Swiss roll manifold with $t$-ranges $[\pi,5\pi/2]$, $[3\pi,7\pi/2]$, and $[4\pi,9\pi/2]$. The blue sprial represents the true swiss roll manifold for which our clusters are generated around.
    }
	\label{Fig:SpiOg}
\end{figure}

We choose to construct three different types of graphs for this example: a K-NN graph with a uniform kernel, a K-NN graph with the RBF kernel, and a proximity graph with the RBF kernel. We tune the parameters for all of these approaches using the method descirbed in the previous section. Graphs formed using each of these techniques is shown in Figure~\ref{Fig:SpiGraphs}.
    \begin{figure}[ht] 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNUni.png} 
    \caption{K-NN with uniform kernel} 
    \label{Fig:SpiKNNUni}
  \end{subfigure}%% 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNRBF.png} 
    \caption{K-NN with RBF kernel} 
    \label{Fig:SpiKNNRBF}
  \end{subfigure} 
    \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiProx.png} 
    \caption{Proximity with RBF kernel} 
    \label{Fig:SpiProx}
  \end{subfigure} 
  \caption{Plot of the graphs formed for the data in Figure~\ref{Fig:SpiOg}. Each subfigure shows a different graph forming technique.}
  \label{Fig:SpiGraphs} 
\end{figure}

We then perform our GSSL algorithm with both the Probit loss function as described in Section~\ref{Sec:Probit} and the regression loss function given in Section~\ref{Sec:3Clus}. Plots of the predicted labels using both the probit and regression loss functions for each type of graph are shown in Figure~\ref{Fig:SpiPred}.
    \begin{figure}[ht] 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNUniPro.png} 
    \caption{K-NN with uniform kernel: Probit} 
    \label{Fig:SpiKNNUniPro} 
  \end{subfigure}%% 
  \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNUniReg.png} 
    \caption{K-NN with uniform kernel: Regression} 
    \label{Fig:SpiKNNUniReg} 
  \end{subfigure} 
    \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNRBFPro.png} 
    \caption{K-NN with RBF kernel: Probit} 
    \label{Fig:SpiKNNRBFPro} 
  \end{subfigure} 
      \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiKNNRBFReg.png} 
    \caption{K-NN with RBF kernel: Regression} 
    \label{Fig:SpiKNNRBFReg} 
  \end{subfigure}
      \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiProxPro.png} 
    \caption{Proximity with RBF kernel: Probit} 
    \label{Fig:SpiProxPro} 
  \end{subfigure}
      \begin{subfigure}{0.475\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/SpiProxReg.png} 
    \caption{Proximity with RBF kernel: Regression} 
    \label{Fig:SpiProxReg} 
  \end{subfigure}
  \caption{Plot of the predicted labels for the clusters generated on the Swiss Roll manifold using the graphs generated in Figure~\ref{Fig:SpiGraphs}. }
  \label{Fig:SpiPred} 
\end{figure}
The accuracies of the predicted labels compared to the true labels for each type of graph and each loss function is shown in Table~\ref{Table:SpiRes}.
\begin{table}
\begin{center}
\begin{tabular}{||c c c c||} 
 \hline
 Type of Graph & Loss Function & 1 Trial Accuracy & 50 Trial Avg. Accuracy\\ 
 \hline\hline
 K-NN with uniform kernel & Probit & 91.00\% & 94.80\%\\ 
 \hline
 K-NN with uniform kernel & Regression & 91.00\% & 93.51\%\\
 \hline
 K-NN with RBF kernel & Probit & 92.25\% & 96.30\%\\
 \hline
 K-NN with RBF kernel & Regression & 92.50\% & 95.76\%\\
 \hline
 Proximity with RBF kernel & Probit & 73.00\% & N/A\\
 \hline
 Proximity with RBF kernel & Regression & 63.00\% & N/A\\
 \hline
\end{tabular}
\caption{Accuracy of predicted labels over 1 run and average accuracy over 50 runs for 3 different types of graphs using both probit and regression loss functions. Note that we did not perform 50 trials for the proximity graph with the RBF kernel as this approach performed very poorly in our initial tests.}
\label{Table:SpiRes}
\end{center}
\end{table}
We can see from Figure~\ref{Fig:SpiPred} and Table~\ref{Table:SpiRes} that the proximity graph with the RBF kernel performs extremely poorly in comparison with both of the K-NN kernels that both have over $90\%$ accuracy. As such, we proceed only with the two K-NN graphs. To ensure the consistency of the success of the K-NN kernels, we computed the average accuracy over 50 trials for the two K-NN graphs with both the probit and regression loss functions. The average accuracy of over these 50 trials are shown in Table~\ref{Table:SpiRes}. 

\subsection{MNIST Data}

\section{Conclusion}


\bibliographystyle{plain}
\bibliography{refs}

\end{document}